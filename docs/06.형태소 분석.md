# 06. 형태소 분석 가이드 🔍

<!-- DOC_STYLE_GUIDE -->
> 📌 <font color="blue"><b>핵심</b></font>: 이 문서는 실무 적용 기준으로 정리되었습니다.
> ⚠️ <font color="red"><b>주의</b></font>: 설정/절차를 생략하면 장애나 품질 저하가 발생할 수 있습니다.
> ✅ <font color="green"><b>권장</b></font>: 체크리스트 순서대로 실행하고 결과를 반드시 검증하세요.

이 문서는 Elasticsearch Nori 기반 형태소 분석이 무엇인지, 그리고 프로젝트 `index-mapping.json` 설정이 실제로 어떤 동작을 만드는지 설명합니다.

## 1. 형태소 분석이란?
- 형태소 분석은 문장을 <font color="blue"><b>의미 단위(토큰)</b></font>로 분해하는 과정입니다.
- 검색에서는 "어떻게 토큰화되었는지"가 검색 정확도에 직접 영향을 줍니다.

예시:
- 입력: `얇은피 만두`
- 기대: `얇은피`, `만두`
- 형태소 분석이 잘못되면 `얇은` + `피`로 분리되어 검색 품질이 흔들릴 수 있습니다.

---

## 2. 프로젝트 적용 설정 (`index-mapping.json` 발췌)

```json
"tokenizer": {
  "ko_nori_userdict_tokenizer": {
    "type": "nori_tokenizer",
    "decompound_mode": "mixed",
    "user_dictionary": "analysis/user_dict_ko.txt"
  }
},
"filter": {
  "ko_pos_filter": {
    "type": "nori_part_of_speech",
    "stoptags": [
      "E",
      "IC",
      "J",
      "MAG",
      "MAJ",
      "MM",
      "SP",
      "SSC",
      "SSO",
      "SC",
      "SE",
      "XPN",
      "XSA",
      "XSN",
      "XSV"
    ]
  }
}
```

---

## 3. tokenizer 설정 상세 설명

### `ko_nori_userdict_tokenizer`
- <font color="blue"><b>한국어 Nori 토크나이저 + 사용자 사전</b></font> 조합입니다.

### `type: nori_tokenizer`
- Elasticsearch의 한국어 형태소 분석 토크나이저를 사용합니다.

### `decompound_mode: mixed`
- 복합명사를 분해 토큰 + 원형 토큰을 함께 유지합니다.
- 예: `손목시계` -> `손목`, `시계`, `손목시계`
- <font color="green"><b>검색 recall(찾아내는 범위)을 높이기 좋은 모드</b></font>입니다.

### `user_dictionary: analysis/user_dict_ko.txt`
- 사용자 정의 사전 경로(ES 컨테이너 `config` 기준 상대경로)입니다.
- 실제 경로는 `/usr/share/elasticsearch/config/analysis/user_dict_ko.txt`로 해석됩니다.
- <font color="red"><b>이 파일이 Pod에 마운트되지 않으면 인덱스 생성/분석 단계에서 실패할 수 있습니다.</b></font>

---

## 4. `ko_pos_filter` 설정 상세 설명

### `type: nori_part_of_speech`
- Nori가 분석한 토큰 중 특정 품사 태그를 제거하는 필터입니다.
- 목적: 검색 품질에 도움되지 않는 조사/어미/부호 등을 제외해 노이즈를 줄임

### `stoptags`
- 제거할 품사 태그 목록입니다.
- 아래 태그는 검색 인덱싱/검색 시점에서 제외됩니다.

| 태그 | 의미(품사) | 제거 이유(실무 관점) |
|---|---|---|
| `E` | 어미(Ending) | 활용/어미는 검색 핵심 의미가 약해 노이즈 가능성이 큼 |
| `IC` | 감탄사(Interjection) | 의미 검색에서 기여도가 낮음 |
| `J` | 조사(Particle) | 문법 기능 위주라 검색 키워드 의미가 약함 |
| `MAG` | 일반 부사(General Adverb) | 수식 성분이 많아 잡음 증가 가능 |
| `MAJ` | 접속 부사(Conjunctive Adverb) | 문장 연결용 성분으로 검색 기여가 낮음 |
| `MM` | 관형사(Determiner) | 수식 성분으로 핵심 명사 대비 기여 낮음 |
| `SP` | 공백(Space) | 토큰 의미 없음 |
| `SSC` | 닫는 괄호(Closing Bracket) | 문장부호 제거 |
| `SSO` | 여는 괄호(Opening Bracket) | 문장부호 제거 |
| `SC` | 구분자(Separator) | 쉼표/중점 등 구분 기호 제거 |
| `SE` | 말줄임표(Ellipsis) | 기호성 토큰 제거 |
| `XPN` | 체언 접두사(Prefix) | 단독 의미 약하고 결합형으로 주로 쓰임 |
| `XSA` | 형용사 파생 접미사 | 파생 성분 노이즈 감소 목적 |
| `XSN` | 명사 파생 접미사 | 파생 성분 노이즈 감소 목적 |
| `XSV` | 동사 파생 접미사 | 파생 성분 노이즈 감소 목적 |

> [!WARNING]
> <font color="red"><b>stoptags를 과도하게 늘리면 검색 recall이 급격히 떨어질 수 있습니다.</b></font>

---

## 5. 운영 시 체크포인트

- <font color="green"><b>사전 변경 후에는 재색인</b></font>이 필요합니다. (tokenizer 규칙 변경)
- `_analyze`로 기대 토큰이 나오는지 먼저 확인합니다.
- POS 필터 변경 시, 대표 검색어(상품명/브랜드명/카테고리명) 회귀 테스트를 수행합니다.

예시 `_analyze` (권장 스크립트):
```bash
 ./sh_bin/91.nori_analyse.sh "얇은피 만두"
{
  "tokens": [
    {
      "token": "만두",
      "start_offset": 0,
      "end_offset": 3,
      "type": "SYNONYM",
      "position": 0
    },
    {
      "token": "교자",
      "start_offset": 0,
      "end_offset": 3,
      "type": "SYNONYM",
      "position": 0
    },
    {
      "token": "얄피",
      "start_offset": 0,
      "end_offset": 3,
      "type": "SYNONYM",
      "position": 0
    },
    {
      "token": "얇은피",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    }
  ]
}



./sh_bin/91.nori_analyse.sh "아버지가방에들어가십니다."  

{
  "tokens": [
    {
      "token": "아버지",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "가방",
      "start_offset": 3,
      "end_offset": 5,
      "type": "word",
      "position": 1
    },
    {
      "token": "들어가",
      "start_offset": 6,
      "end_offset": 9,
      "type": "word",
      "position": 3
    }
  ]
}

./sh_bin/91.nori_analyse.sh "아버지가 방에 들어가십니다."

{
  "tokens": [
    {
      "token": "아버지",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "방",
      "start_offset": 5,
      "end_offset": 6,
      "type": "word",
      "position": 2
    },
    {
      "token": "들어가",
      "start_offset": 8,
      "end_offset": 11,
      "type": "word",
      "position": 4
    }
  ]
}

```


## 10. `_analyze` 결과 요약 표 🧪

아래는 `./sh_bin/91.nori_analyse.sh` 실행 결과를 간략히 비교한 표입니다.

| 입력 문장 | 주요 토큰(순서) | 특징 |
|---|---|---|
| `얇은피 만두` | `만두`, `교자`, `얄피`, `얇은피` | 동의어(`SYNONYM`)가 함께 확장됨 |
| `아버지가방에들어가십니다.` | `아버지`, `가방`, `들어가` | 띄어쓰기 없음: `가방`으로 분석됨 |
| `아버지가 방에 들어가십니다.` | `아버지`, `방`, `들어가` | 띄어쓰기 있음: `방`으로 분석됨 |

<font color="blue"><b>핵심 포인트: 형태소 분석은 띄어쓰기/사전/동의어 설정에 따라 토큰 결과가 달라집니다.</b></font>


스크립트 동작:
- Kubernetes Secret에서 비밀번호를 자동 조회:
  - `kubectl get secret ai-search-es-es-elastic-user -n ai-search ...`
- 형태소 분석 대상 텍스트는 첫 번째 인자로 전달
- 기본값:
  - `ES_URL=http://localhost:9200`
  - `ANALYZE_INDEX=food-products-read`
  - `ANALYZER_NAME=ko_mall_search_analyzer`
  - `NAMESPACE=ai-search`
  - `SECRET_NAME=ai-search-es-es-elastic-user`

## 6. 한 줄 결론
<font color="blue"><b>형태소 분석 품질은 검색 품질의 기반이며, `user_dictionary` + `stoptags`는 품질과 노이즈의 균형을 조정하는 핵심 레버입니다.</b></font>
