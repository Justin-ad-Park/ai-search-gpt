# 검색 임베딩 모델 

<!-- DOC_STYLE_GUIDE -->
> 📌 <font color="#FFD700"><b>핵심</b></font>: 이 문서는 실무 적용 기준으로 정리되었습니다.
> ⚠️ <font color="red"><b>주의</b></font>: 설정/절차를 생략하면 장애나 품질 저하가 발생할 수 있습니다.
> ✅ <font color="green"><b>권장</b></font>: 체크리스트 순서대로 실행하고 결과를 반드시 검증하세요.

이 문서는 본 프로젝트에서 사용하는 임베딩 모델과 설정 방법을 설명합니다.
검색 기획자와 검색 개발자가 함께 이해할 수 있도록, 목적/효과/운영 관점까지 정리했습니다.

## 0. 왜 임베딩이라고 하나?
1️⃣ 원래 영어 의미
 - embed = “끼워 넣다”, “내부에 심다”
 - 예:
   - 보석을 반지에 끼워 넣다 
   - 영상에 자막을 삽입하다 (embed subtitles)
 - 즉, <font color="#FFD700"><b>어떤 것을 다른 공간 안에 자리 잡게 만드는 것</font>

### AI에서 임베딩?
- 텍스트, 이미지 같은 데이터를 숫자 공간(vector space) 안에 자리 잡게 만드는 것
- 조금 더 정확히 말하면 <font color="#FFD700"><b>의미(텍스트, 이미지)를 수치 좌표로 변환해서 수학적으로 비교 가능하게 만드는 과정</font>

## 1. 임베딩 모델이 하는 일
임베딩 모델은 **문장을 숫자 벡터로 바꾸는 모델**입니다.
이 벡터를 기반으로 “의미가 가까운 상품”을 찾는 것이 벡터 검색의 핵심입니다.

- 입력: 사용자의 검색어 또는 상품 설명 텍스트
- 출력: 고정 길이의 숫자 벡터
- 사용 위치: Elasticsearch <font color="#FFD700"><b>`dense_vector`</b></font> 필드에 저장 및 <font color="#FFD700"><b>`knn`</b></font> 검색에 사용

즉, **문자열을 “의미 좌표”로 바꿔주는 변환기**라고 생각하면 됩니다.

### `dense_vector`란?
- <font color="#FFD700"><b>Dense Vector는 AI가 텍스트를 숫자 좌표로 바꾼 값이며, Elasticsearch에서 의미 기반 검색을 가능하게 해주는 필드 타입</b></font>이다.
- Elasticsearch에서 Dense Vector는 숫자 배열(벡터) 형태로 데이터를 저장하는 필드 타입입니다. 
- 주로 AI 임베딩(embedding) 값을 저장해서 의미 기반 검색(semantic search) 을 할 때 사용됩니다.
- Dense Vector는:
  - 고정된 차원 수 (예: 768차원)
  - 모든 차원에 값이 존재함 (0이 거의 없음)
  - 그래서 “조밀한(dense)” 벡터라고 부름. <br><br>
- 예: `[0.12, -0.03, ...]` 같은 숫자 배열을 문서와 함께 저장합니다.
- 이 프로젝트에서는 상품명/설명에서 만든 임베딩을 이 필드에 넣습니다.

### `knn`이란?
- <font color="#FFD700"><b>k-nearest neighbors</b></font>의 약자이며, "가장 가까운 벡터 k개"를 찾는 검색 방식입니다.
- 사용자의 검색어 벡터와 문서 벡터의 거리를 계산해 유사한 순서로 결과를 반환합니다.
- 키워드 일치가 없어도 의미가 비슷하면 검색될 수 있어, <font color="green"><b>의미 기반 검색 품질</b></font>을 높입니다.


## 기존 검색 엔진과 차이 
- 기존의 검색 엔진은 확률적 정보 검색 이론(BM25)에 기반하여 널리 사용되는 텍스트 검색 알고리즘을 사용합니다. 
- 이 알고리즘은 용어의 빈도에 따라 문서의 순위를 매기며, 용어빈도(TF, Term Frequency 해당 단어가 문서와 관련이 높다는 것), 역문서 빈도(IDF, Inverse Document Frequency - 어떤 단어가 전체 문서 집합에서 얼마나 “희귀한지”를 나타내는 값입니다. 변별력을 높임), 문서 길이 정규화 등과 같은 요소를 이용합니다.
- 정확한 용어 일치에 크게 의존하기 때문에 동의어, 철자 오류 또는 미묘한 시맨틱 변화를 처리할 때 관련성이 떨어지는 결과를 초래할 수 있습니다.
- 단어 간의 문맥 관계를 파악하지 못하기 때문에 구문이나 문장의 의미를 이해하는데 효과적이지 못합니다.

## 벡터 검색 
- 벡터는 단어, 구문 또는 전체 문서 사이의 시맨틱과 맥락적 관계를 표현합니다.
- 벡터 검색은 용어의 관련성을 판단합니다. 그 결과 벡터 검색은 동의어, 철자 오류 또는 다른 구문이 포함된 질의에 대해 더 관련성 높은 결과를 도출할 수 있습니다.
- 즉, 벡터 검색은 자연어의 복잡성을 더 잘 이해하고 이에 대응할 수 있는 보다 섬세한 접근 방식을 제공합니다.

### 벡터 검색의 적용 예
- 이커머스 제품 검색 
- 문서 검색
- 지식 관리 시스템 : 검색어에 사용된 정확한 단어가 문서에 나타나지 않더라도 유사한 내용, 문맥 또는 주제를 가진 문서를 찾을 수 있어 지식 관리 시스템에 적합
- 이미지 인식 및 검색 : 시각적으로 유사한 이미지를 찾을 수 있어. 유사 이미지 검색, 이미지 중복 감지, 이미지 추천 시스템에 활용
- 음악 추천 : 오디오를 벡터로 표현하면 유사한 특징이나 스타일을 가진 음악을 식별할 수 있습니다.
- 보안/행동 분석 : 잠재적인 위협, 이상 징후 또는 악성 활동을 탐지할 수 있습니다. 전통적인 방법으로 식별하기 어려운 것들을 찾아내 전반적인 보안 수준을 높일 수 있습니다. 

## 벡터 알고리즘의 큰 축 
### 1️⃣KNN (k-Nearest Neighbors)
- 👉 “가장 가까운 k개를 찾는 방법” 자체를 의미
- K = 몇 개를 찾을 것인지 (예: 상위 10개)
- Nearest = 거리 기준으로 가장 가까운 것
- Neighbors = 이웃 데이터

즉,
- “쿼리 벡터와 가장 가까운 k개의 벡터를 찾는다”
- 거리 계산 방식:
- 유클리드 거리, 코사인 유사도, 내적(dot product)

📌 특징
정확함 (Exact Search), 모든 벡터와 비교, 데이터가 많아지면 느려짐

### 2️⃣ ANN (Approximate Nearest Neighbor)
- 👉 “가장 가까운 것과 거의 비슷한 것”을 빠르게 찾는 방법 
- Approximate = 근사치

즉,
- 100% 정확한 최근접 이웃이 아니라
- "거의 가까운 것"을 훨씬 빠르게 찾음

📌 특징
매우 빠름, 약간의 오차 허용, 대규모 벡터 검색에 적합


## 1-1. DJL이란?
DJL(Deep Java Library)은 **Java에서 딥러닝 모델을 쉽게 사용하기 위한 라이브러리**입니다.
이 프로젝트에서는 DJL이 다음 역할을 합니다.

- Hugging Face에 있는 임베딩 모델을 다운로드/로딩
- 텍스트를 임베딩 벡터로 변환(추론)

### 🤗 Hugging Face 는 뭐야?
- Hugging Face는 AI 모델을 공유하고 다운로드할 수 있는 오픈 AI 모델 플랫폼이야.
- 📦 AI 모델의 GitHub + App Store 같은 곳 
- 즉, **Java 코드에서 AI 모델을 직접 불러 쓰게 해주는 “연결 고리”**라고 이해하면 됩니다.

## 2. 본 프로젝트의 사용 모델
현재 기본 모델은 다음과 같습니다.

- 모델: 
  - `dragonkue/multilingual-e5-small-ko-v2`
    - 로컬 컴파일 기반으로 모델 내장(속도 빠름, 운영 안정적)
  - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
    - URL 기반으로 원격으로 모델 다운로드(개발/운영 난이도 낮음)
  - 지원 언어: 다국어(한국어 포함)
  - 특성: 가볍고 빠른 모델, 의미 유사도 검색에 적합

설정 위치는 `application.yml`의 다음 항목입니다.

```yaml
  ## **  embedding-model-path 우선 적용하며, 없으면 model-url 사용함  **
  embedding-model-path: ${AI_SEARCH_EMBED_MODEL_PATH:classpath:/model/multilingual-e5-small-ko-v2}
  embedding-model-url: ${AI_SEARCH_EMBED_MODEL:djl://ai.djl.huggingface.pytorch/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2}
```

이 값이 **임베딩 모델의 “실제 출처”를 가리키는 문자열**이며, DJL이 이 값을 해석해서 모델을 다운로드/로딩합니다.

## 3. 설정 값(embedding-model-url)의 의미
`embedding-model-url`은 **DJL이 지원하는 모델 로딩 경로**입니다.

현재 사용 값의 의미는 다음과 같습니다.

- `djl://` : DJL 전용 모델 경로 스킴
- `ai.djl.huggingface.pytorch` : Hugging Face의 PyTorch 모델 저장소 사용
- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` : 실제 모델 이름

- 다국어를 지원하는 paraphrase-multilingual-MiniLM-L12-v2의 경우 유사도 표현이 좀 더 보수적인 경향이 눈에 띕니다.

- paraphrase-multilingual-MiniLM-L6-v2
![all-MiniLM-L6-v2](./images/all-MiniLM-L6-v2.png)

- paraphrase-multilingual-MiniLM-L12-v2
![all-MiniLM-L12-v2](./images/MiniLM-L12-v2.png)


- 사랑-우정 사이의 유사도 하락이 그 예시죠.
- all-MiniLM-L6-v2의 경우에는 "The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised contrastive learning objective" 라고 밝히고 있습니다. 목적 또한 데이터셋을 찾아보면 일반적인 영어 문장에 최적화 되어있음을 밝힙니다.

https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

이 값을 바꾸면 **임베딩 모델을 교체할 수 있습니다**. 코드 수정 없이 설정만 바꾸는 구조입니다.

## 4. 검색 기획 관점에서의 의미
임베딩 모델은 **“검색 품질의 방향성”**을 결정합니다.

- 더 정확한 모델: 의미 유사도는 좋아지지만 속도/비용이 증가할 수 있음
- 더 가벼운 모델: 속도는 빠르지만 의미 매칭이 덜 섬세할 수 있음

따라서 **검색 기획 단계에서 요구하는 품질/속도/운영 비용 기준**이
모델 선택에 직접 영향을 줍니다.

예시:
- “간식”과 “스낵”을 같은 의미로 이해해야 한다 → 의미 유사도 모델 필요
- 검색 응답 속도가 매우 중요하다 → 경량 모델 필요

## 5. 검색 개발 관점에서의 의미
검색 개발자는 **모델 차원 수, 성능, 인덱싱 비용**을 고려해야 합니다.

- 임베딩 차원 수가 늘면 인덱스 용량이 증가합니다.
- 모델이 무거우면 인덱싱 시간/검색 응답이 느려집니다.
- <font color="red"><b>모델이 바뀌면 인덱스를 다시 만들어야 합니다.</b></font>

즉, 모델 변경은 **검색 품질 + 운영 비용**에 직접 영향을 줍니다.

## 6. 이 프로젝트에서 모델이 쓰이는 흐름
1. `application.yml`에서 `embedding-model-url`을 읽음
2. `DjlEmbeddingService`가 해당 URL로 모델을 로딩
3. 상품 텍스트와 검색어를 임베딩 벡터로 변환
4. Elasticsearch에 벡터 저장 + `knn` 검색 수행

관련 코드 위치:
- `src/main/java/com/example/aisearch/service/embedding/DjlEmbeddingService.java`
- `src/main/resources/application.yml`

## 7. 모델 교체 시 체크리스트
모델을 바꿀 경우 아래 항목을 꼭 확인해야 합니다.

- 모델이 한국어 의미 검색에 충분한지
- 모델 크기(다운로드/메모리)가 운영 환경에 적합한지
- 임베딩 차원 수가 달라졌는지
- 인덱스를 재생성해야 하는지
- 테스트 검색 품질이 기대 수준인지

## 8. 추천 운영 방식
- 개발 초기에는 현재 모델처럼 가벼운 모델로 빠르게 반복
- 품질 이슈가 확인되면 더 강한 모델로 교체 검토
- 모델 교체 시 검색 품질 지표(클릭/전환)와 응답 시간 지표를 같이 평가



## [참고] RAG
RAG는 Retrieval-Augmented Generation의 약자입니다.
- Retrieval: 필요한 정보를 검색해서 가져오고
- Augmented: 그 정보를 LLM 입력에 보강해서
- Generation: 그걸 바탕으로 답변을 생성하는 방식

